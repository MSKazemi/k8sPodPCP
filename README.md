# EnergetiScope ‚Äî Quick Start for Scheduler Integrations

EnergetiScope is a FastAPI service to estimate power/energy for Kubernetes workloads. Use it from schedulers, controllers, or CronJobs to predict energy in advance.

## In-cluster access

- Service DNS: `http://energetiscope-predict.<namespace>.svc.cluster.local:8000`
- Default port: `8000`
- Deploy with `k8s/*deploy-energetiscope-predict*.yaml` (ensures model/encoder artifacts are mounted)

## Endpoints

- POST `/predict`
  - Input (JSON): `InferenceRequest` (same schema produced by `app/k8s_collect.py`)
  - Output (JSON):
    - `pred_energy_step_j` (float)
    - `workload_kind`, `workload_name`, `namespace`, `spec_hash`

- POST `/infer/from-yaml`
  - Input (text/plain): Kubernetes YAML for `Deployment`/`Job`/`CronJob`
  - Output (JSON): Array of `InferenceRequest` JSON objects

- POST `/predict/from-yaml`
  - Input (text/plain): Kubernetes YAML for `Deployment`/`Job`/`CronJob`
  - Output (JSON): Same shape as `/predict`

Swagger UI: `http://<service>/docs` ‚Ä¢ OpenAPI: `http://<service>/openapi.json`

## Minimal examples

### Predict from a live Deployment (inside cluster)

```bash
kubectl get deploy nginx -n default -o yaml | \
curl -sS -X POST \
  http://energetiscope-predict.default.svc.cluster.local:8000/predict/from-yaml \
  -H 'Content-Type: text/plain' --data-binary @- | jq .
```

### Predict with `InferenceRequest` JSON

```bash
curl -sS -X POST http://energetiscope-predict.default.svc.cluster.local:8000/predict \
  -H 'Content-Type: application/json' \
  -d '{
        "workload_kind": "Deployment",
        "workload_name": "nginx",
        "namespace": "default",
        "pod_spec": {"containers": [{"name": "nginx", "image": "nginx:1.25"}]}
      }' | jq .
```

### Response shape

```json
{
  "pred_energy_step_j": 123.4,
  "workload_kind": "Deployment",
  "workload_name": "nginx",
  "namespace": "default",
  "spec_hash": "6c7c..."
}
```

Full documentation moved to `README_FULL.md`.

## ‚ú® Features

- **Kubernetes watcher**: Streams `Deployment`/`Job`/`CronJob`/`Pod` specs to structured `InferenceRequest` JSON
- **Feature encoder**: SBERT-optional text embeddings + numeric/categorical features ‚Üí Parquet with vectors
- **Label builder (Kepler/Prometheus)**: Exports `avg_power_w` and `energy_step_j`/`total_energy_j` labels
- **Training**: KNN baseline with group-aware cross-validation; saves `.joblib` model
- **Serving**: FastAPI `POST /predict` endpoint that returns predicted energy for a workload spec
- **Kubernetes manifests**: Jobs for collect‚Üíencode‚Üílabel‚Üíjoin‚Üítrain and Deployments for API + collector

## üß± Architecture

```mermaid
flowchart TD
  subgraph Cluster
    Collector[Collector
    k8s_collect.py] -->|POST InferenceRequest| API[
    FastAPI: /predict]
    API --> |uses| Encoder[(encoder.joblib)]
    API --> |uses| Model[(knn_energy.joblib)]
    Kepler[(Kepler)] -->|metrics| Prometheus[(Prometheus)]
  end
  Jobs[Batch Jobs
  collect‚Üíencode‚Üílabel‚Üíjoin‚Üítrain] -->|artifacts| PVC[(energetiscope-data PVC)]
  API --> PVC
  Collector --> PVC
```

## üöÄ Quickstart

### Prerequisites

- Python ‚â• 3.11
- Access to a Kubernetes cluster (optional for local dev)
- Prometheus + Kepler for label generation (see `kepler.md` and `prometheus.md`)
- Docker (optional) and `kubectl`/`helm` if deploying to Kubernetes

### Local Install

```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
```

### Run API locally

```bash
# from repo root
uvicorn app/predict_service:app --host 0.0.0.0 --port 8000
# open http://localhost:8000/docs
```

### FastAPI endpoints (app/predict_service.py)

- **POST `/predict`**: Accepts an `InferenceRequest` JSON (same schema produced by `app/k8s_collect.py`) and returns a `PredictOut` with `pred_energy_step_j` and metadata.
- **POST `/infer/from-yaml`**: Accepts Kubernetes YAML (`Content-Type: text/plain`) for `Deployment`/`Job`/`CronJob` and returns the derived `InferenceRequest` JSON(s).
- **POST `/predict/from-yaml`**: Accepts Kubernetes YAML (`Content-Type: text/plain`) and returns a `PredictOut` directly.

Service auto-loads artifacts at startup using env vars:

- `ENCODER_PATH` (default: `/artifacts/encoder.joblib`)
- `MODEL_PATH` (default: `/artifacts/knn_energy.joblib`)

Swagger UI is available at `/docs` and OpenAPI JSON at `/openapi.json`.

### Local CLI workflow (collect ‚Üí encode ‚Üí label ‚Üí join ‚Üí train ‚Üí predict)

```bash
# 1) Collect workload templates to NDJSON (from your current cluster)
python app/k8s_collect.py watch \
  --kinds Deployment Job CronJob Pod \
  --emit-initial \
  --suppress-tls-warnings \
  --output data/in.ndjson

# 2) Fit encoder (disable SBERT initially for speed)
python app/k8s_encode.py fit \
  --input data/in.ndjson \
  --out artifacts/encoder.joblib \
  --no-sbert

# 3) Transform features
python app/k8s_encode.py transform \
  --input data/in.ndjson \
  --encoder artifacts/encoder.joblib \
  --out data/features.parquet

# 4) Export labels from Kepler/Prometheus (job-level aggregation window)
python app/kepler_labels.py \
  --prom http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090 \
  --mode job \
  --start $(date -u -d '6 hours ago' +%s) \
  --end   $(date -u +%s) \
  --out data/labels.parquet

# 5) Join features ‚Üî labels
python app/join_features_labels.py \
  --features data/features.parquet \
  --labels   data/labels.parquet \
  --out      data/train_rows.parquet

# 6) Train KNN model (total energy as target)
python app/train_power.py \
  --train data/train_rows.parquet \
  --target total_energy_j \
  --out artifacts/knn_energy.joblib

# 7) Predict offline for current workloads
python app/predict_k8s.py \
  --encoder artifacts/encoder.joblib \
  --model   artifacts/knn_energy.joblib \
  --input   data/in.ndjson
```

### Docker

```bash
docker build -t energetiscope:latest .

# Run API (override image CMD to start uvicorn)
docker run --rm -p 8000:8000 \
  -e ENCODER_PATH=/app/artifacts/encoder.joblib \
  -e MODEL_PATH=/app/artifacts/knn_energy.joblib \
  -v "$(pwd)/app/artifacts:/app/artifacts:ro" \
  energetiscope:latest \
  uvicorn app/predict_service:app --host 0.0.0.0 --port 8000
```

### Kubernetes

Install Prometheus and Kepler first (see `prometheus.md` and `kepler.md`). Then run the training pipeline jobs and deploy the services:

```bash
# Namespace + PVC
kubectl apply -f k8s/jobs/00-ns-pvc.yaml
# Maybe need to update storage class to local-path or

# RBAC for reading cluster objects
kubectl apply -f k8s/jobs/01-rbac.yaml

# Job 1: collect + encode
kubectl apply -f k8s/jobs/02-job1-collect.yaml

# Job 2: build labels from Kepler
kubectl apply -f k8s/jobs/03-job2-label.yaml
# maybe need to update the prometheus endpoint


# Job 3: join features and labels
kubectl apply -f k8s/jobs/04-job3-dataset.yaml

# Job 4: train model
kubectl apply -f k8s/jobs/05-job4-train.yaml

# Deploy predictor API (Service + optional Ingress)
kubectl apply -f k8s/deploy-energetiscope-predict.yaml

# Deploy collector (watches cluster and optionally POSTs to API)
kubectl apply -f k8s/deploy-energetiscope-collector.yaml
```

#### Example: CronJob that periodically pre-scores a Deployment

Use a `CronJob` to predict energy for an existing Deployment on a schedule by piping the live manifest into the API. Ensure the Service name (`energetiscope-predict` below) matches your install, and the ServiceAccount has RBAC to read the target resource.

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: energetiscope-prescore
  namespace: default
spec:
  schedule: "*/10 * * * *"  # every 10 minutes
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: energetiscope-reader
          restartPolicy: OnFailure
          containers:
          - name: prescore
            image: bitnami/kubectl:1.29
            command: ["/bin/sh","-c"]
            args:
            - |
              DEPLOY=nginx-deployment
              NS=default
              kubectl get deploy ${DEPLOY} -n ${NS} -o yaml | \
              curl -sS -X POST http://energetiscope-predict.default.svc.cluster.local:8000/predict/from-yaml \
                -H 'Content-Type: text/plain' --data-binary @- | jq .
```

Notes:

- Replace `energetiscope-predict.default.svc.cluster.local:8000` with your Service DNS/port.
- Create a minimal RBAC allowing `get` on `deployments` in the target namespace for `energetiscope-reader`.

## ‚öôÔ∏è Configuration

| Variable       | Default                         | Description                                  |
| -------------- | -------------------------------- | -------------------------------------------- |
| `ENCODER_PATH` | `/artifacts/encoder.joblib`      | Encoder artifact path (service default)       |
| `MODEL_PATH`   | `/artifacts/knn_energy.joblib`   | Trained model path (service default)          |
| `KUBECONFIG`   | autodetect                       | Path for out-of-cluster k8s client (collector) |
| `K8S_CA_FILE`  | empty                            | CA file path for TLS (collector)              |
| `VERIFY_SSL`   | client default                   | Force SSL verify on/off (collector)           |

Notes:
- In Kubernetes, the manifests mount a PVC at `/app/artifacts` and set env vars to use `/app/artifacts/*.joblib`.
- The FastAPI service loads artifacts at startup; ensure the PVC contains the trained files.

## üìö Usage Examples

- **HTTP API**

```bash
curl -s http://localhost:8000/predict \
  -H 'Content-Type: application/json' \
  -d @sample.json | jq .
```

- **Collector from a YAML file**

```bash
python app/k8s_collect.py from-file ./example/example.yaml
```

- **Prometheus / Kepler sanity checks**
  - Ensure Prom has data for: `kube_pod_owner`, `kepler_container_power_watt`, `kepler_container_joules_total`

- **Python (use in schedulers/workflows)**

  ```python
  import requests

  # Predict directly from a Kubernetes YAML manifest
  yaml_text = open("example/example.yaml", "r", encoding="utf-8").read()
  r = requests.post(
      "http://localhost:8000/predict/from-yaml",
      data=yaml_text,
      headers={"Content-Type": "text/plain"},
      timeout=30,
  )
  r.raise_for_status()
  print(r.json())  # {'pred_energy_step_j': ..., 'workload_kind': 'Deployment', ...}

  # Or send an InferenceRequest JSON directly to /predict
  ir = {
      # Minimal example; prefer generating with app/k8s_collect.py
      "workload_kind": "Deployment",
      "workload_name": "nginx",
      "namespace": "default",
      "pod_spec": {
          "containers": [{"name": "nginx", "image": "nginx:1.25"}]
      }
  }
  r2 = requests.post(
      "http://localhost:8000/predict",
      json=ir,
      timeout=30,
  )
  r2.raise_for_status()
  print(r2.json())
  ```

- **Response shape**

  ```json
  {
    "pred_energy_step_j": 123.4,
    "workload_kind": "Deployment",
    "workload_name": "nginx",
    "namespace": "default",
    "spec_hash": "6c7c..."
  }
  ```

## üß™ Testing & Quality

> **TODO:** Add tests and CI. Suggested commands once added:
> `pytest -q`, `ruff check .`, `black --check .`

## üó∫Ô∏è Roadmap

- Add validation datasets and benchmarks
- Optional mutating webhook to annotate workloads with predicted power
- Model registry and automated retraining

## ü§ù Contributing

Contributions are welcome. Please open an issue to discuss major changes.

## üîê Security

Please report vulnerabilities via a private issue.

## üßæ License

> **TODO:** Add a `LICENSE` file (recommended: Apache-2.0 or MIT).

## üìù Cite Us

> **DOI:** `TODO`

```bibtex
@misc{EnergetiScope2025,
  title = {EnergetiScope: Machine Learning-Based Energy Prediction for Kubernetes Workloads},
  author = {TODO},
  year = {2025},
  doi = {TODO}
}
```

## üôè Acknowledgments

- Kepler (`sustainable-computing-io/kepler`)
- Prometheus & kube-prometheus-stack

## üì¨ Contact

`<CONTACT_NAME>` ‚Äî `<EMAIL>`

## üì¶ Additional Artifacts

- `kepler.md`: Helm install commands for Kepler
- `prometheus.md`: Helm install commands for kube-prometheus-stack
- `example/example.yaml`: Sample K8s manifest for local tryouts

